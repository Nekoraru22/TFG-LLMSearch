from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import chromadb


def crear_embeddings(textos):
    """
    Creates embeddings for a list of texts using Sentence Transformers.
    
    Args:
        textos: List of strings with texts to process
    
    Returns:
        list: List of vector embeddings
    """
    modelo = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = modelo.encode(textos)
    return embeddings


def crear_chroma_db(collection_name="my_collection", persist_directory="./chroma_db"):
    """
    Creates a ChromaDB instance and a collection.
    
    Args:
        collection_name (str): Name of the collection
        persist_directory (str): Directory where the data will be persistently stored
    
    Returns:
        chromadb.Collection: Chroma collection object
    """
    # Create a Chroma client with disk persistence
    cliente = chromadb.PersistentClient(path=persist_directory)
    print(f"Chroma database configured to be stored in: {persist_directory}")
    
    # Create a collection
    try:
        # Attempt to retrieve the collection if it already exists
        collection = cliente.get_collection(collection_name)
        print(f"Collection '{collection_name}' already exists, using the existing one.")
    except Exception:
        # Create a new collection if it does not exist
        collection = cliente.create_collection(collection_name)
        print(f"Collection '{collection_name}' created successfully.")
    
    return collection


def agregar_documentos(collection, documentos, embeddings=None, metadatos=None, ids=None):
    """
    Adds documents to a Chroma collection.
    
    Args:
        collection (chromadb.Collection): Chroma collection
        documentos (list): List of documents (strings)
        embeddings (list, optional): List of embeddings. If None, they are generated automatically.
        metadatos (list, optional): List of metadata (dictionaries) for each document
        ids (list, optional): List of IDs for each document. If None, they are generated automatically.
    """
    if ids is None:
        ids = [f"doc_{i}" for i in range(len(documentos))]
    
    if metadatos is None:
        metadatos = [{"source": "example"} for _ in range(len(documentos))]
    
    if embeddings is None:
        # Use embeddings automatically generated by Chroma
        collection.add(
            documents=documentos,
            metadatas=metadatos,
            ids=ids
        )
    else:
        # Use custom embeddings
        collection.add(
            documents=documentos,
            embeddings=embeddings,
            metadatas=metadatos,
            ids=ids
        )


def buscar_similares(collection, query, n_results=3, embeddings=None):
    """
    Searches for documents similar to a query in the Chroma collection.
    
    Args:
        collection (chromadb.Collection): Chroma collection
        query (str): Query text
        n_results (int): Number of results to return
        embeddings (np.array, optional): Embedding of the query. If None, it is generated automatically.
    
    Returns:
        dict: Search results
    """
    if embeddings is None:
        resultados = collection.query(
            query_texts=[query],
            n_results=n_results
        )
    else:
        resultados = collection.query(
            query_embeddings=[embeddings],
            n_results=n_results
        )
    
    return resultados


def visualizar_embeddings_3d(embeddings, labels=None, title="3D Embeddings Visualization", highlight_index=None, figsize=(10, 8)):
    """
    Visualizes embeddings in a 3D space using PCA to reduce dimensionality.
    
    Args:
        embeddings (np.array): Embeddings matrix
        labels (list, optional): List of labels for each point
        title (str): Title of the plot
        highlight_index (int, optional): Index of the point to highlight
        figsize (tuple): Figure size (width, height)
    """
    # Reduce dimensionality to 3D using PCA
    pca = PCA(n_components=3)
    embeddings_3d = pca.fit_transform(embeddings)
    
    # Create a 3D figure
    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111, projection='3d')
    
    # Base colors for all points
    colors = ['blue'] * len(embeddings_3d)
    sizes = [30] * len(embeddings_3d)
    
    # If there is a point to highlight, change it to red and make it larger
    if highlight_index is not None:
        colors[highlight_index] = 'red'
        sizes[highlight_index] = 100
    
    # Add labels if provided
    if labels is not None:
        for i, (x, y, z) in enumerate(embeddings_3d):
            ax.text(x, y, z, labels[i], size=8, zorder=1, color='black')
    
    # Add explained variance information for each component
    explained_variance = pca.explained_variance_ratio_
    ax.set_xlabel(f'PC1 ({explained_variance[0]:.2%})')
    ax.set_ylabel(f'PC2 ({explained_variance[1]:.2%})')
    ax.set_zlabel(f'PC3 ({explained_variance[2]:.2%})')  # type: ignore
    
    # Title
    ax.set_title(title)
    
    # Add connecting lines from the highlighted point to the others
    if highlight_index is not None:
        punto_destacado = embeddings_3d[highlight_index]
        for i, punto in enumerate(embeddings_3d):
            if i != highlight_index:
                ax.plot([punto_destacado[0], punto[0]], 
                        [punto_destacado[1], punto[1]], 
                        [punto_destacado[2], punto[2]], 
                        'gray', alpha=0.3, linestyle=':')
                
                # Calculate Euclidean distance
                distancia = np.linalg.norm(embeddings[highlight_index] - embeddings[i])
                # Midpoint to show the distance
                punto_medio = (punto_destacado + punto) / 2
                ax.text(punto_medio[0], punto_medio[1], punto_medio[2], f'{distancia:.2f}', size=8, color='red')  # type: ignore
    
    plt.tight_layout()
    return fig


def visualizar_matriz_distancias(embeddings, labels=None, figsize=(10, 8)):
    """
    Visualizes a distance matrix between embeddings.
    
    Args:
        embeddings (np.array): Embeddings matrix
        labels (list, optional): List of labels for each embedding
        figsize (tuple): Figure size (width, height)
    """
    # Calculate the distance matrix
    n = len(embeddings)
    dist_matrix = np.zeros((n, n))
    
    for i in range(n):
        for j in range(n):
            dist_matrix[i, j] = np.linalg.norm(embeddings[i] - embeddings[j])
    
    # Create a DataFrame for better visualization
    if labels is None:
        labels = [f"Doc {i}" for i in range(n)]
    
    df_dist = pd.DataFrame(dist_matrix, index=labels, columns=labels)
    
    # Visualize the distance matrix as a heatmap
    fig, ax = plt.subplots(figsize=figsize)
    im = ax.imshow(dist_matrix, cmap='viridis')
    
    # Add a color bar
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel("Euclidean Distance", rotation=-90, va="bottom")
    
    # Add labels
    ax.set_xticks(np.arange(len(labels)))
    ax.set_yticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)
    ax.set_yticklabels(labels)
    
    # Rotate labels for better visualization
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # Add distance values in each cell
    for i in range(n):
        for j in range(n):
            ax.text(j, i, f"{dist_matrix[i, j]:.2f}",
                           ha="center", va="center", color="white" if dist_matrix[i, j] > dist_matrix.max()/2 else "black")
    
    ax.set_title("Distance Matrix Between Embeddings")
    fig.tight_layout()
    
    return fig, df_dist


def prove() -> None:
    # Create some example documents
    documentos = [
        "Python is a high-level, interpreted programming language",
        "Embeddings are vector representations of text",
        "Chroma is a vector database for storing embeddings",
        "Language models can generate semantic embeddings",
        "3D visualization helps to understand the distance between embeddings",
        "Vector databases are useful for semantic searches",
        "Embeddings capture the semantics of words and phrases",
        "Python has many libraries for natural language processing"
    ]
    
    # Create embeddings for the documents
    embeddings = crear_embeddings(documentos)
    
    # Create a Chroma collection with persistence
    collection = crear_chroma_db("example_embeddings", "./data/chroma_db")
    
    # Add documents with custom embeddings
    metadatos = [{"type": "definition", "source": "example"} for _ in range(len(documentos))]
    ids = [f"doc_{i}" for i in range(len(documentos))]
    agregar_documentos(collection, documentos, embeddings, metadatos, ids)
    
    # Perform a search
    query = "What are embeddings?"
    resultados = buscar_similares(collection, query, n_results=3)
    
    print("\nSearch results for:", query)
    if resultados['documents'] is not None and resultados["distances"] is not None:
        for i, doc in enumerate(resultados['documents'][0]):
            print(f"{i+1}. {doc} (Distance: {resultados['distances'][0][i]:.4f})")
    else:
        print("No similar documents were found.")
    
    # Visualize embeddings in 3D
    labels = [f"Doc {i}: {doc[:20]}..." for i, doc in enumerate(documentos)]
    
    # Visualize with the query point highlighted
    query_embedding = crear_embeddings([query])[0]
    all_embeddings = np.vstack([embeddings, query_embedding])
    all_labels = labels + [f"Query: {query}"]
    
    fig = visualizar_embeddings_3d(
        embeddings=all_embeddings,
        labels=all_labels, 
        title="3D Embeddings Visualization with Query",
        highlight_index=len(embeddings)
    )
    
    # Visualize the distance matrix
    fig_matriz, df_dist = visualizar_matriz_distancias(embeddings, labels)
    
    plt.show()
    
    print("\nDistance Matrix:")
    print(df_dist)
