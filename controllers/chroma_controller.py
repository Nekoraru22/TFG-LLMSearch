from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import chromadb

class ChromaClient:
    """
    Class to manage a Chroma client with persistent storage.
    """

    def __init__(self, persist_directory="./chroma_db"):
        """
        Initializes a Chroma client with a specified persistence directory.
        
        Args:
            persist_directory: Directory where the data will be persistently stored
        """
        self.persist_directory = persist_directory
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = None
        
        print(f"Chroma database configured to be stored in: {persist_directory}")


    def create_chroma_collection(self, collection_name="my_collection") -> None:
        """
        Creates a ChromaDB collection.
        
        Args:
            collection_name: Name of the collection
        
        Returns:
            chromadb.Collection: Chroma collection object
        """
        try:
            # Attempt to retrieve the collection if it already exists
            self.collection = self.client.get_collection(collection_name)
            print(f"Collection '{collection_name}' already exists, using the existing one.")
        except Exception:
            # Create a new collection if it does not exist
            self.collection = self.client.create_collection(collection_name)
            print(f"Collection '{collection_name}' created successfully.")


    def create_embeddings(self, texts):
        """
        Creates embeddings for a list of texts using Sentence Transformers.
        
        Args:
            texts: List of strings with texts to process
        
        Returns:
            list: List of vector embeddings
        """
        modelo = SentenceTransformer(model_name_or_path='all-MiniLM-L6-v2')
        embeddings = modelo.encode(str(texts))
        return embeddings


    def add_documents(self, documents: list, embeddings=None, metadatas=None, ids=None) -> None:
        """
        Adds documents to a Chroma collection.
        
        Args:
            documents: List of documents (strings)
            embeddings: List of embeddings. If None, they are generated automatically.
            metadatas: List of metadata (dictionaries) for each document
            ids: List of IDs for each document. If None, they are generated automatically.
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{"source": "example"} for _ in range(len(documents))]  # Ensure keys and values match Metadata type

        if embeddings is None:
            # Use embeddings automatically generated by Chroma
            self.collection.add(
                documents=documents,
                metadatas=[{k: v for k, v in metadata.items()} for metadata in metadatas],
                ids=ids
            )
        else:
            # Use custom embeddings
            self.collection.add(
                documents=documents,
                embeddings=embeddings,
                metadatas=[{k: v for k, v in metadata.items()} for metadata in metadatas],
                ids=ids
            )


    def search_similar(self, query: str, n_results: int = 3, embeddings = None):
        """
        Searches for documents similar to a query in the Chroma collection.
        
        Args:
            query: Query text
            n_results: Number of results to return
            embeddings: Embedding of the query. If None, it is generated automatically.
        
        Returns:
            dict: Search results
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")

        if embeddings is None:
            resultados = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )
        else:
            resultados = self.collection.query(
                query_embeddings=[embeddings],
                n_results=n_results
            )
        
        return resultados


    def visualize_embeddings_3d(self, embeddings, labels=None, title="3D Embeddings Visualization", highlight_index=None, figsize=(10, 8)):
        """
        Visualizes embeddings in a 3D space using PCA to reduce dimensionality.
        
        Args:
            embeddings (np.array): Embeddings matrix
            labels (list, optional): List of labels for each point
            title (str): Title of the plot
            highlight_index (int, optional): Index of the point to highlight
            figsize (tuple): Figure size (width, height)
        """
        # Reduce dimensionality to 3D using PCA
        pca = PCA(n_components=3)
        embeddings_3d = pca.fit_transform(embeddings)
        
        # Create a 3D figure
        fig = plt.figure(figsize=figsize)
        ax = fig.add_subplot(111, projection='3d')
        
        # Base colors for all points
        colors = ['blue'] * len(embeddings_3d)
        sizes = [30] * len(embeddings_3d)
        
        # If there is a point to highlight, change it to red and make it larger
        if highlight_index is not None:
            colors[highlight_index] = 'red'
            sizes[highlight_index] = 100
        
        # Add labels if provided
        if labels is not None:
            for i, (x, y, z) in enumerate(embeddings_3d):
                ax.text(x, y, z, labels[i], size=8, zorder=1, color='black')
        
        # Add explained variance information for each component
        explained_variance = pca.explained_variance_ratio_
        ax.set_xlabel(f'PC1 ({explained_variance[0]:.2%})')
        ax.set_ylabel(f'PC2 ({explained_variance[1]:.2%})')
        ax.set_zlabel(f'PC3 ({explained_variance[2]:.2%})')  # type: ignore
        
        # Title
        ax.set_title(title)
        
        # Add connecting lines from the highlighted point to the others
        if highlight_index is not None:
            punto_destacado = embeddings_3d[highlight_index]
            for i, punto in enumerate(embeddings_3d):
                if i != highlight_index:
                    ax.plot([punto_destacado[0], punto[0]], 
                            [punto_destacado[1], punto[1]], 
                            [punto_destacado[2], punto[2]], 
                            'gray', alpha=0.3, linestyle=':')
                    
                    # Calculate Euclidean distance
                    distancia = np.linalg.norm(embeddings[highlight_index] - embeddings[i])
                    # Midpoint to show the distance
                    punto_medio = (punto_destacado + punto) / 2
                    ax.text(punto_medio[0], punto_medio[1], punto_medio[2], f'{distancia:.2f}', size=8, color='red')  # type: ignore
        
        plt.tight_layout()
        return fig


    def visualize_matriz_distances(self, embeddings, labels=None, figsize=(10, 8)):
        """
        Visualizes a distance matrix between embeddings.
        
        Args:
            embeddings (np.array): Embeddings matrix
            labels (list, optional): List of labels for each embedding
            figsize (tuple): Figure size (width, height)
        """
        # Calculate the distance matrix
        n = len(embeddings)
        dist_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                dist_matrix[i, j] = np.linalg.norm(embeddings[i] - embeddings[j])
        
        # Create a DataFrame for better visualization
        if labels is None:
            labels = [f"Doc {i}" for i in range(n)]
        
        df_dist = pd.DataFrame(dist_matrix, index=labels, columns=labels)
        
        # Visualize the distance matrix as a heatmap
        fig, ax = plt.subplots(figsize=figsize)
        im = ax.imshow(dist_matrix, cmap='viridis')
        
        # Add a color bar
        cbar = ax.figure.colorbar(im, ax=ax)
        cbar.ax.set_ylabel("Euclidean Distance", rotation=-90, va="bottom")
        
        # Add labels
        ax.set_xticks(np.arange(len(labels)))
        ax.set_yticks(np.arange(len(labels)))
        ax.set_xticklabels(labels)
        ax.set_yticklabels(labels)
        
        # Rotate labels for better visualization
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
        
        # Add distance values in each cell
        for i in range(n):
            for j in range(n):
                ax.text(j, i, f"{dist_matrix[i, j]:.2f}",
                            ha="center", va="center", color="white" if dist_matrix[i, j] > dist_matrix.max()/2 else "black")
        
        ax.set_title("Distance Matrix Between Embeddings")
        fig.tight_layout()
        
        return fig, df_dist


def prove() -> None:
    chroma = ChromaClient(persist_directory="./data/chroma_db")

    # Create some example documents
    documentos = [
        "Python is a high-level, interpreted programming language",
        "Embeddings are vector representations of text",
        "Chroma is a vector database for storing embeddings",
        "Language models can generate semantic embeddings",
        "3D visualization helps to understand the distance between embeddings",
        "Vector databases are useful for semantic searches",
        "Embeddings capture the semantics of words and phrases",
        "Python has many libraries for natural language processing"
    ]
    
    # Create embeddings for the documents
    embeddings = chroma.create_embeddings(documentos)
    
    # Create a Chroma collection with persistence
    chroma.create_chroma_collection("example_embeddings")
    
    # Add documents with custom embeddings
    metadatos = [{"type": "definition", "source": "example"} for _ in range(len(documentos))]
    ids = [f"doc_{i}" for i in range(len(documentos))]
    chroma.add_documents(documentos, embeddings, metadatos, ids)
    
    # Perform a search
    query = "What are embeddings?"
    resultados = chroma.search_similar(query, n_results=3)
    
    print("\nSearch results for:", query)
    if resultados['documents'] is not None and resultados["distances"] is not None:
        for i, doc in enumerate(resultados['documents'][0]):
            print(f"{i+1}. {doc} (Distance: {resultados['distances'][0][i]:.4f})")
    else:
        print("No similar documents were found.")
    
    # Visualize embeddings in 3D
    labels = [f"Doc {i}: {doc[:20]}..." for i, doc in enumerate(documentos)]
    
    # Visualize with the query point highlighted
    query_embedding = chroma.create_embeddings([query])[0]
    all_embeddings = np.vstack([embeddings, query_embedding])
    all_labels = labels + [f"Query: {query}"]
    
    fig = chroma.visualize_embeddings_3d(
        embeddings=all_embeddings,
        labels=all_labels, 
        title="3D Embeddings Visualization with Query",
        highlight_index=len(embeddings)
    )
    
    # Visualize the distance matrix
    fig_matriz, df_dist = chroma.visualize_matriz_distances(embeddings, labels)
    
    plt.show()
    
    print("\nDistance Matrix:")
    print(df_dist)
