import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import chromadb
import os

from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from utils import get_file_hash


class ChromaClient:
    """
    Class to manage a Chroma client with persistent storage.
    """

    def __init__(self, persist_directory="./chroma_db"):
        """
        Initializes a Chroma client with a specified persistence directory.
        
        Args:
            persist_directory: Directory where the data will be persistently stored
        """
        self.persist_directory = persist_directory
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = None
        
        print(f"Chroma database configured to be stored in: {persist_directory}")


    def create_chroma_collection(self, collection_name="my_collection") -> None:
        """
        Creates a ChromaDB collection.
        
        Args:
            collection_name: Name of the collection
        
        Returns:
            chromadb.Collection: Chroma collection object
        """
        try:
            # Attempt to retrieve the collection if it already exists
            self.collection = self.client.get_collection(collection_name)
            print(f"Collection '{collection_name}' already exists, using the existing one.")
        except Exception:
            # Create a new collection if it does not exist
            self.collection = self.client.create_collection(collection_name)
            print(f"Collection '{collection_name}' created successfully.")


    def create_embeddings(self, texts):
        """
        Creates embeddings for a list of texts using Sentence Transformers.
        
        Args:
            texts: List of strings with texts to process
        
        Returns:
            list: List of vector embeddings
        """
        modelo = SentenceTransformer(model_name_or_path='all-MiniLM-L6-v2')
        embeddings = modelo.encode(texts)
        return embeddings


    def add_or_update_documents(self, documents: list, embeddings=None, metadatas=None, ids=None) -> None:
        """
        Adds documents to a Chroma collection.
        
        Args:
            documents: List of documents (strings)
            embeddings: List of embeddings. If None, they are generated automatically.
            metadatas: List of metadata (dictionaries) for each document
            ids: List of IDs for each document. If None, they are generated automatically.
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{"source": "example"} for _ in range(len(documents))]  # Ensure keys and values match Metadata type

        if embeddings is None:
            # Use embeddings automatically generated by Chroma
            self.collection.upsert(
                documents=documents,
                metadatas=[{k: v for k, v in metadata.items()} for metadata in metadatas],
                ids=ids
            )
        else:
            # Use custom embeddings
            self.collection.upsert(
                documents=documents,
                embeddings=embeddings,
                metadatas=[{k: v for k, v in metadata.items()} for metadata in metadatas],
                ids=ids
            )


    def delete_documents(self, path: str) -> None:
        """
        Deletes documents from the Chroma collection based on the file path.
        
        Args:
            path: Path of the file to delete
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        # Delete the document with the specified path
        self.collection.delete(
            where={"path": path}
        )


    def search_similar(self, query: str, n_results: int = 3, embeddings = None):
        """
        Searches for documents similar to a query in the Chroma collection.
        
        Args:
            query: Query text
            n_results: Number of results to return
            embeddings: Embedding of the query. If None, it is generated automatically.
        
        Returns:
            dict: Search results
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")

        if embeddings is None:
            resultados = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )
        else:
            resultados = self.collection.query(
                query_embeddings=[embeddings],
                n_results=n_results
            )
        
        return resultados


    def check_duplicate(self, path: str) -> bool:
        """
        Comprueba si el hash del archivo en 'path' ya existe en la colecciÃ³n de Chroma.

        Args:
            path: Ruta del archivo a comprobar.

        Returns:
            True si el hash ya existe, False en caso contrario.
        """
        file_hash = get_file_hash(path)

        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        results = self.collection.get(
            where={"hash": file_hash}
        )
        print(f"Hash: {file_hash}")
        print(f"Duplicate search results: {results}")

        # 3. Si hay al menos un id devuelto, el hash ya existe
        return len(results["ids"]) > 0


    def visualize_embeddings_3d(self, embeddings, labels=None, title="3D Embeddings Visualization", highlight_index=None, figsize=(10, 8)):
        """
        Visualizes embeddings in a 3D space using PCA to reduce dimensionality.
        
        Args:
            embeddings (np.array): Embeddings matrix
            labels (list, optional): List of labels for each point
            title (str): Title of the plot
            highlight_index (int, optional): Index of the point to highlight
            figsize (tuple): Figure size (width, height)
        """
        # Reduce dimensionality to 3D using PCA
        pca = PCA(n_components=3)
        embeddings_3d = pca.fit_transform(embeddings)
        
        # Create a 3D figure
        fig = plt.figure(figsize=figsize)
        ax = fig.add_subplot(111, projection='3d')
        
        # Base colors for all points
        colors = ['blue'] * len(embeddings_3d)
        sizes = [30] * len(embeddings_3d)
        
        # If there is a point to highlight, change it to red and make it larger
        if highlight_index is not None:
            colors[highlight_index] = 'red'
            sizes[highlight_index] = 100
        
        # Add labels if provided
        if labels is not None:
            for i, (x, y, z) in enumerate(embeddings_3d):
                ax.text(x, y, z, labels[i], size=8, zorder=1, color='black')
        
        # Add explained variance information for each component
        explained_variance = pca.explained_variance_ratio_
        ax.set_xlabel(f'PC1 ({explained_variance[0]:.2%})')
        ax.set_ylabel(f'PC2 ({explained_variance[1]:.2%})')
        ax.set_zlabel(f'PC3 ({explained_variance[2]:.2%})')  # type: ignore
        
        # Title
        ax.set_title(title)
        
        # Add connecting lines from the highlighted point to the others
        if highlight_index is not None:
            punto_destacado = embeddings_3d[highlight_index]
            for i, punto in enumerate(embeddings_3d):
                if i != highlight_index:
                    ax.plot([punto_destacado[0], punto[0]], 
                            [punto_destacado[1], punto[1]], 
                            [punto_destacado[2], punto[2]], 
                            'gray', alpha=0.3, linestyle=':')
                    
                    # Calculate Euclidean distance
                    distancia = np.linalg.norm(embeddings[highlight_index] - embeddings[i])
                    # Midpoint to show the distance
                    punto_medio = (punto_destacado + punto) / 2
                    ax.text(punto_medio[0], punto_medio[1], punto_medio[2], f'{distancia:.2f}', size=8, color='red')  # type: ignore
        
        plt.tight_layout()
        return fig


    def visualize_matriz_distances(self, embeddings, labels=None, figsize=(10, 8)):
        """
        Visualizes a distance matrix between embeddings.
        
        Args:
            embeddings (np.array): Embeddings matrix
            labels (list, optional): List of labels for each embedding
            figsize (tuple): Figure size (width, height)
        """
        # Calculate the distance matrix
        n = len(embeddings)
        dist_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                dist_matrix[i, j] = np.linalg.norm(embeddings[i] - embeddings[j])
        
        # Create a DataFrame for better visualization
        if labels is None:
            labels = [f"Doc {i}" for i in range(n)]
        
        df_dist = pd.DataFrame(dist_matrix, index=labels, columns=labels)
        
        # Visualize the distance matrix as a heatmap
        fig, ax = plt.subplots(figsize=figsize)
        im = ax.imshow(dist_matrix, cmap='viridis')
        
        # Add a color bar
        cbar = ax.figure.colorbar(im, ax=ax)
        cbar.ax.set_ylabel("Euclidean Distance", rotation=-90, va="bottom")
        
        # Add labels
        ax.set_xticks(np.arange(len(labels)))
        ax.set_yticks(np.arange(len(labels)))
        ax.set_xticklabels(labels)
        ax.set_yticklabels(labels)
        
        # Rotate labels for better visualization
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
        
        # Add distance values in each cell
        for i in range(n):
            for j in range(n):
                ax.text(j, i, f"{dist_matrix[i, j]:.2f}",
                            ha="center", va="center", color="white" if dist_matrix[i, j] > dist_matrix.max()/2 else "black")
        
        ax.set_title("Distance Matrix Between Embeddings")
        fig.tight_layout()
        
        return fig, df_dist


    def get_desc_with_path(self, path: str) -> dict:
        """
        Recupera un dict con la ruta (path), su descripciÃ³n y metadatos
        desde la colecciÃ³n de ChromaDB.

        Args:
            path: Ruta del fichero a recuperar (debe existir en metadatos).

        Returns:
            Un dict con keys "path", "description" y "metadata".
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        # Filter documents by path
        result = self.collection.get(
            where={"path": path},
            limit=1
        )

        if not result["ids"]:
            raise ValueError(f"No entry found in ChromaDB for path: {path}")

        # Return the first document and its metadata
        doc = result["documents"]
        meta = result["metadatas"]

        return {
            "description": doc,
            "metadata": meta
        }


    def get_all_paths(self) -> list:
        """
        Retrieves all paths stored in the Chroma collection.

        Returns:
            A list of paths.
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        # Get all documents in the collection
        results = self.collection.get()
        
        # Extract paths from the metadata
        paths = [metadata["path"] for metadata in results["metadatas"]] if results["metadatas"] else []
        
        return paths
    

    def get_all_document_names(self) -> list:
        """
        Retrieves all document names stored in the Chroma collection.

        Returns:
            A list of document names.
        """
        if self.collection is None:
            raise ValueError("Chroma collection not initialized. Please create a collection first.")
        
        # Get all documents in the collection
        results = self.collection.get()
        
        # Extract document names from the metadata
        document_names = [metadata for metadata in results["metadatas"]] if results["metadatas"] else []
        
        return document_names


def create_graphics(query: str, debug: bool = False) -> None: 
    chroma = ChromaClient(persist_directory=str(os.environ.get("CHROMA_DB_PATH"))) 
    chroma.create_chroma_collection(collection_name=str(os.environ.get("CHROMA_COLLECTION_NAME"))) 
 
    # Perform a search 
    resultados = chroma.search_similar(query, n_results=3) 
     
    if debug:
        print("\nSearch results for:", query)
        if resultados['documents'] is not None and resultados["distances"] is not None:
            for i, doc in enumerate(resultados['documents'][0]):
                metadata = resultados['metadatas'][0][i] if 'metadatas' in resultados and resultados['metadatas'] is not None else {}
                print(f"{i+1}. {doc} (Distance: {resultados['distances'][0][i]:.4f})")
                print(f"Metadata: {metadata}")
        else:
            print("No similar documents were found.")
     
    # Visualize embeddings in 3D
    # Usar directamente la colecciÃ³n almacenada en el atributo collection de ChromaClient
    collection = chroma.collection
    if collection is None:
        raise ValueError("Chroma collection not initialized. Please create a collection first.")
    
    # Obtener documentos de la colecciÃ³n
    collection_data = collection.get()
    documents = collection_data['documents']
    
    if not documents:
        raise ValueError("No documents found in the collection.")
    
    # Crear embeddings para todos los documentos y la consulta usando el mismo modelo
    print(f"Creating embeddings for {len(documents)} documents and the query...")
    modelo = SentenceTransformer(model_name_or_path='all-MiniLM-L6-v2')
    document_embeddings = modelo.encode(documents)
    query_embedding = modelo.encode([query])
    
    # Verificar dimensiones
    print(f"Document embeddings shape: {document_embeddings.shape}")
    print(f"Query embedding shape: {query_embedding.shape}")
    
    # Crear etiquetas para los documentos
    labels = [f"Doc {i}: {doc[:20]}..." for i, doc in enumerate(documents)]
    
    # Combinar embeddings de documentos con el embedding de la consulta
    all_embeddings = np.vstack([document_embeddings, query_embedding])
    all_labels = labels + [f"Query: {query}"]
    
    # Visualizar embeddings en 3D con el punto de consulta resaltado
    fig = chroma.visualize_embeddings_3d(
        embeddings=all_embeddings,
        labels=all_labels,
        title="3D Embeddings Visualization with Query",
        highlight_index=document_embeddings.shape[0]
    )
    
    # Visualizar matriz de distancias
    fig_matriz, df_dist = chroma.visualize_matriz_distances(document_embeddings, labels)
    
    plt.show()
    
    if debug:
        print("\nDistance Matrix:")
        print(df_dist)


def prove() -> None:
    chroma = ChromaClient(persist_directory="./data/chroma_prove_db")

    # Create some example documents
    documentos = [
        "Python is a high-level, interpreted programming language",
        "Embeddings are vector representations of text",
        "Chroma is a vector database for storing embeddings",
        "Language models can generate semantic embeddings",
        "3D visualization helps to understand the distance between embeddings",
        "Vector databases are useful for semantic searches",
        "Embeddings capture the semantics of words and phrases",
        "Python has many libraries for natural language processing"
    ]
    
    # Create embeddings for the documents
    embeddings = chroma.create_embeddings(documentos)
    
    # Create a Chroma collection with persistence
    chroma.create_chroma_collection("example_embeddings")
    
    # Add documents with custom embeddings
    chroma.add_or_update_documents(documentos, embeddings)
    
    # Perform a search
    query = "What are embeddings?"
    resultados = chroma.search_similar(query, n_results=3)
    
    print("\nSearch results for:", query)
    if resultados['documents'] is not None and resultados["distances"] is not None:
        for i, doc in enumerate(resultados['documents'][0]):
            print(f"{i+1}. {doc} (Distance: {resultados['distances'][0][i]:.4f})")
    else:
        print("No similar documents were found.")
    
    # Visualize embeddings in 3D
    labels = [f"Doc {i}: {doc[:20]}..." for i, doc in enumerate(documentos)]
    
    # Visualize with the query point highlighted
    query_embedding = chroma.create_embeddings([query])[0]
    all_embeddings = np.vstack([embeddings, query_embedding])
    all_labels = labels + [f"Query: {query}"]
    
    fig = chroma.visualize_embeddings_3d(
        embeddings=all_embeddings,
        labels=all_labels, 
        title="3D Embeddings Visualization with Query",
        highlight_index=len(embeddings)
    )
    
    # Visualize the distance matrix
    fig_matriz, df_dist = chroma.visualize_matriz_distances(embeddings, labels)
    
    plt.show()
    
    print("\nDistance Matrix:")
    print(df_dist)